# -*- coding: utf-8 -*-
"""CODVEDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UEl-6kN2luYHcUCQgd_CoW-CO3MVdOeK

# **Iris Dataset Linear Regression Analysis**
# Level 2 (Intermediate) - Task 1: Regression Analysis
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualization
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("="*60)
print("IRIS DATASET LINEAR REGRESSION ANALYSIS")
print("="*60)

#load dataset
# Pastikan file iris.csv sudah di-upload di Google Colab
df = pd.read_csv("/content/1) iris.csv")
df

# Display basic information about the dataset
print(f"\nDataset Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print("\nFirst 5 rows:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nBasic Statistics:")
print(df.describe())

print("\nSpecies Distribution:")
print(df['species'].value_counts())

# Check for missing values
print(f"\nMissing Values:\n{df.isnull().sum()}")

print("\n" + "="*60)
print("EXPLORATORY DATA ANALYSIS")
print("="*60)

# Create correlation matrix for numerical features
numerical_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
correlation_matrix = df[numerical_features].corr()

plt.figure(figsize=(15, 12))

# Subplot 1: Correlation Heatmap
plt.subplot(2, 3, 1)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5)
plt.title('Feature Correlation Matrix', fontsize=12, fontweight='bold')

# Subplot 2: Pairplot-like scatter plots
features_to_plot = ['sepal_length', 'petal_length', 'petal_width']
for i, feature in enumerate(features_to_plot, 2):
    plt.subplot(2, 3, i)
    for species in df['species'].unique():
        species_data = df[df['species'] == species]
        plt.scatter(species_data['sepal_width'], species_data[feature],
                   label=species, alpha=0.7, s=50)
    plt.xlabel('Sepal Width')
    plt.ylabel(feature.replace('_', ' ').title())
    plt.title(f'Sepal Width vs {feature.replace("_", " ").title()}')
    plt.legend()
    plt.grid(True, alpha=0.3)

# Distribution plots
plt.subplot(2, 3, 5)
for species in df['species'].unique():
    species_data = df[df['species'] == species]
    plt.hist(species_data['sepal_length'], alpha=0.7, label=species, bins=15)
plt.xlabel('Sepal Length')
plt.ylabel('Frequency')
plt.title('Distribution of Sepal Length by Species')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 3, 6)
df.boxplot(column='petal_length', by='species', ax=plt.gca())
plt.title('Petal Length Distribution by Species')
plt.suptitle('')  # Remove default title

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("LINEAR REGRESSION ANALYSIS")
print("="*60)

# We'll perform multiple regression analyses to predict different variables
# Let's start with predicting petal_length based on other features

print("🎯 REGRESSION TASK 1: Predicting Petal Length")
print("-" * 50)

# Prepare features and target
# We'll use sepal_length, sepal_width, and petal_width to predict petal_length
X = df[['sepal_length', 'sepal_width', 'petal_width']]
y = df['petal_length']

print(f"Features (X): {list(X.columns)}")
print(f"Target (y): petal_length")
print(f"Dataset size: {X.shape[0]} samples, {X.shape[1]} features")

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=df['species']
)

print(f"\nTraining set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# Fit the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

print("\n✅ Linear Regression Model Trained Successfully!")

# Make predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calculate evaluation metrics
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

print("\n" + "="*50)
print("MODEL EVALUATION METRICS")
print("="*50)

print(f"📊 TRAINING SET PERFORMANCE:")
print(f"   • R-squared (R²): {train_r2:.4f}")
print(f"   • Mean Squared Error (MSE): {train_mse:.4f}")
print(f"   • Root Mean Squared Error (RMSE): {np.sqrt(train_mse):.4f}")
print(f"   • Mean Absolute Error (MAE): {train_mae:.4f}")

print(f"\n📊 TESTING SET PERFORMANCE:")
print(f"   • R-squared (R²): {test_r2:.4f}")
print(f"   • Mean Squared Error (MSE): {test_mse:.4f}")
print(f"   • Root Mean Squared Error (RMSE): {np.sqrt(test_mse):.4f}")
print(f"   • Mean Absolute Error (MAE): {test_mae:.4f}")

# Model interpretation - coefficients
print(f"\n🔍 MODEL COEFFICIENTS (Feature Importance):")
print(f"   • Intercept: {model.intercept_:.4f}")
for feature, coef in zip(X.columns, model.coef_):
    print(f"   • {feature}: {coef:.4f}")

print(f"\n📝 INTERPRETATION:")
print(f"   • The model explains {test_r2*100:.1f}% of the variance in petal length")
if test_r2 > 0.8:
    print(f"   • This indicates a STRONG relationship between features and petal length")
elif test_r2 > 0.6:
    print(f"   • This indicates a MODERATE relationship between features and petal length")
else:
    print(f"   • This indicates a WEAK relationship between features and petal length")

# Visualization of results
plt.figure(figsize=(15, 10))

# Subplot 1: Actual vs Predicted (Training)
plt.subplot(2, 3, 1)
plt.scatter(y_train, y_train_pred, alpha=0.7, color='blue', s=50)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
plt.xlabel('Actual Petal Length')
plt.ylabel('Predicted Petal Length')
plt.title(f'Training Set: Actual vs Predicted\nR² = {train_r2:.3f}')
plt.grid(True, alpha=0.3)

# Subplot 2: Actual vs Predicted (Testing)
plt.subplot(2, 3, 2)
plt.scatter(y_test, y_test_pred, alpha=0.7, color='green', s=50)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Petal Length')
plt.ylabel('Predicted Petal Length')
plt.title(f'Testing Set: Actual vs Predicted\nR² = {test_r2:.3f}')
plt.grid(True, alpha=0.3)

# Subplot 3: Residuals plot (Training)
plt.subplot(2, 3, 3)
residuals_train = y_train - y_train_pred
plt.scatter(y_train_pred, residuals_train, alpha=0.7, color='blue', s=50)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Petal Length')
plt.ylabel('Residuals')
plt.title('Training Set: Residuals Plot')
plt.grid(True, alpha=0.3)

# Subplot 4: Residuals plot (Testing)
plt.subplot(2, 3, 4)
residuals_test = y_test - y_test_pred
plt.scatter(y_test_pred, residuals_test, alpha=0.7, color='green', s=50)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Petal Length')
plt.ylabel('Residuals')
plt.title('Testing Set: Residuals Plot')
plt.grid(True, alpha=0.3)

# Subplot 5: Feature importance
plt.subplot(2, 3, 5)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': np.abs(model.coef_)
})
feature_importance = feature_importance.sort_values('coefficient', ascending=True)
plt.barh(feature_importance['feature'], feature_importance['coefficient'])
plt.xlabel('Absolute Coefficient Value')
plt.title('Feature Importance (|Coefficients|)')
plt.grid(True, alpha=0.3)

# Subplot 6: Distribution of residuals
plt.subplot(2, 3, 6)
plt.hist(residuals_test, bins=15, alpha=0.7, color='green', edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Test Residuals')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("ADDITIONAL REGRESSION ANALYSIS")
print("="*60)

# Let's try another regression: predicting sepal_length
print("🎯 REGRESSION TASK 2: Predicting Sepal Length")
print("-" * 50)

X2 = df[['sepal_width', 'petal_length', 'petal_width']]
y2 = df['sepal_length']

X2_train, X2_test, y2_train, y2_test = train_test_split(
    X2, y2, test_size=0.2, random_state=42, stratify=df['species']
)

model2 = LinearRegression()
model2.fit(X2_train, y2_train)

y2_train_pred = model2.predict(X2_train)
y2_test_pred = model2.predict(X2_test)

test_r2_2 = r2_score(y2_test, y2_test_pred)
test_mse_2 = mean_squared_error(y2_test, y2_test_pred)

print(f"📊 RESULTS:")
print(f"   • R-squared (R²): {test_r2_2:.4f}")
print(f"   • Mean Squared Error (MSE): {test_mse_2:.4f}")
print(f"   • RMSE: {np.sqrt(test_mse_2):.4f}")

print(f"\n🔍 MODEL COEFFICIENTS:")
print(f"   • Intercept: {model2.intercept_:.4f}")
for feature, coef in zip(X2.columns, model2.coef_):
    print(f"   • {feature}: {coef:.4f}")

print("\n" + "="*60)
print("MODEL COMPARISON SUMMARY")
print("="*60)

comparison_data = {
    'Model': ['Predicting Petal Length', 'Predicting Sepal Length'],
    'R²': [test_r2, test_r2_2],
    'MSE': [test_mse, test_mse_2],
    'RMSE': [np.sqrt(test_mse), np.sqrt(test_mse_2)]
}

comparison_df = pd.DataFrame(comparison_data)
print(comparison_df.to_string(index=False, float_format='%.4f'))

print(f"\n🏆 BEST MODEL: {comparison_data['Model'][np.argmax(comparison_data['R²'])]}")
print(f"   (Highest R² = {max(comparison_data['R²']):.4f})")

print("\n" + "="*60)
print("KEY INSIGHTS & RECOMMENDATIONS")
print("="*60)

print("📈 FINDINGS:")
print("   1. The iris dataset shows strong linear relationships between features")
print("   2. Petal measurements tend to be more predictable than sepal measurements")
print("   3. The models achieve good performance with R² > 0.8 in most cases")
print("   4. Feature engineering or polynomial features might improve performance")

print("\n🔧 TECHNICAL NOTES:")
print("   • Data split: 80% training, 20% testing")
print("   • Stratified sampling used to maintain species distribution")
print("   • No feature scaling applied (not necessary for interpretation)")
print("   • Residuals analysis suggests linear assumptions are reasonable")

print("\n✅ ANALYSIS COMPLETE!")
print("="*60)

"""# **Iris Classification using Random Forest and Multiple Models**
# Level 3 (Advanced) - Predictive Modeling Task (Classification)

"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

print("=== IRIS CLASSIFICATION - LEVEL 3 (ADVANCED) ===")
print("Task 1: Predictive Modeling (Classification)\n")

# 1. LOAD AND EXPLORE THE DATA
print("1. LOADING AND EXPLORING DATA")
print("-" * 40)

# Load the iris dataset
# Note: In Google Colab, you would upload the CSV file or use the built-in iris dataset
from sklearn.datasets import load_iris

# Load iris data
iris_data = load_iris()
df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
df['species'] = iris_data.target_names[iris_data.target]

# Display basic information about the dataset
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nSpecies Distribution:")
print(df['species'].value_counts())

print("\nBasic Statistics:")
print(df.describe())

# 2. DATA PREPROCESSING
print("\n2. DATA PREPROCESSING")
print("-" * 40)

# Check for missing values
print("Missing Values:")
print(df.isnull().sum())

# Handle categorical variables (encode species)
le = LabelEncoder()
df['species_encoded'] = le.fit_transform(df['species'])

# Feature scaling
features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
X = df[features]
y = df['species_encoded']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=features)

print("Features after scaling (first 5 rows):")
print(X_scaled.head())

# 3. DATA VISUALIZATION
print("\n3. DATA VISUALIZATION")
print("-" * 40)

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Pairplot-style scatter plots
for i, feature in enumerate(features):
    ax = axes[i//2, i%2]
    for species in df['species'].unique():
        species_data = df[df['species'] == species]
        ax.scatter(species_data[feature], species_data['petal length (cm)'],
                  label=species, alpha=0.7)
    ax.set_xlabel(feature)
    ax.set_ylabel('Petal Length (cm)')
    ax.set_title(f'{feature} vs Petal Length')
    ax.legend()

plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = X.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Heatmap')
plt.show()

# 4. SPLIT THE DATA
print("\n4. DATA SPLITTING")
print("-" * 40)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y)

print(f"Training set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

# 5. TRAIN MULTIPLE CLASSIFICATION MODELS
print("\n5. TRAINING MULTIPLE CLASSIFICATION MODELS")
print("-" * 40)

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
}

# Train and evaluate models
model_results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Store results
    model_results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'predictions': y_pred
    }

    print(f"{name} Results:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

# 6. MODEL EVALUATION AND COMPARISON
print("\n6. MODEL EVALUATION AND COMPARISON")
print("-" * 40)

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'Model': list(model_results.keys()),
    'Accuracy': [result['accuracy'] for result in model_results.values()],
    'Precision': [result['precision'] for result in model_results.values()],
    'Recall': [result['recall'] for result in model_results.values()],
    'F1-Score': [result['f1_score'] for result in model_results.values()]
})

print("Model Comparison:")
print(comparison_df.round(4))

# Visualize model comparison
fig, axes = plt.subplots(1, 4, figsize=(20, 5))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

for i, metric in enumerate(metrics):
    axes[i].bar(comparison_df['Model'], comparison_df[metric])
    axes[i].set_title(f'{metric} Comparison')
    axes[i].set_ylabel(metric)
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Best performing model
best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']
print(f"\nBest performing model: {best_model_name}")

# 7. DETAILED EVALUATION FOR BEST MODEL (Random Forest)
print("\n7. DETAILED EVALUATION - RANDOM FOREST")
print("-" * 40)

rf_model = model_results['Random Forest']['model']
rf_pred = model_results['Random Forest']['predictions']

# Classification Report
print("Classification Report for Random Forest:")
print(classification_report(y_test, rf_pred, target_names=le.classes_))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, rf_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature Importance
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance, x='importance', y='feature')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.show()

print("Feature Importance:")
print(feature_importance)

# 8. HYPERPARAMETER TUNING FOR RANDOM FOREST
print("\n8. HYPERPARAMETER TUNING - RANDOM FOREST")
print("-" * 40)

# Define parameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print("Performing Grid Search for Random Forest...")
print("Parameter grid:", param_grid)

# Perform grid search
rf_grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

rf_grid_search.fit(X_train, y_train)

# Best parameters and score
print(f"\nBest parameters: {rf_grid_search.best_params_}")
print(f"Best cross-validation score: {rf_grid_search.best_score_:.4f}")

# Evaluate tuned model
tuned_rf = rf_grid_search.best_estimator_
tuned_rf_pred = tuned_rf.predict(X_test)

tuned_accuracy = accuracy_score(y_test, tuned_rf_pred)
tuned_precision = precision_score(y_test, tuned_rf_pred, average='weighted')
tuned_recall = recall_score(y_test, tuned_rf_pred, average='weighted')
tuned_f1 = f1_score(y_test, tuned_rf_pred, average='weighted')

print(f"\nTuned Random Forest Results:")
print(f"  Accuracy: {tuned_accuracy:.4f}")
print(f"  Precision: {tuned_precision:.4f}")
print(f"  Recall: {tuned_recall:.4f}")
print(f"  F1-Score: {tuned_f1:.4f}")

# 9. CROSS-VALIDATION
print("\n9. CROSS-VALIDATION")
print("-" * 40)

# Perform cross-validation for all models
cv_results = {}

for name, result in model_results.items():
    model = result['model']
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')
    cv_results[name] = {
        'mean_score': cv_scores.mean(),
        'std_score': cv_scores.std(),
        'scores': cv_scores
    }

    print(f"{name} - CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# Cross-validation for tuned Random Forest
tuned_cv_scores = cross_val_score(tuned_rf, X_scaled, y, cv=5, scoring='accuracy')
print(f"Tuned Random Forest - CV Accuracy: {tuned_cv_scores.mean():.4f} (+/- {tuned_cv_scores.std()*2:.4f})")

# 10. FINAL SUMMARY
print("\n10. FINAL SUMMARY")
print("=" * 50)

print("OBJECTIVES COMPLETED:")
print("✓ Preprocessed the data (handled categorical variables, feature scaling)")
print("✓ Trained and tested multiple classification models:")
print("  - Decision Trees")
print("  - Logistic Regression")
print("  - Random Forest")
print("✓ Evaluated models using accuracy, precision, recall, and F1-score")
print("✓ Performed hyperparameter tuning using grid search")
print("✓ Used tools: Python, scikit-learn, pandas, matplotlib")

print(f"\nBEST MODEL: {best_model_name}")
print(f"Test Accuracy: {model_results[best_model_name]['accuracy']:.4f}")
print(f"After hyperparameter tuning: {tuned_accuracy:.4f}")

print("\nThe iris classification task has been completed successfully!")
print("All species can be classified with high accuracy using the trained models.")

"""# **Sentimen Dataset**
# Task 3: Natural Language Processing (NLP) - Sentiment Analysis
"""

# 1. Install dan import library yang diperlukan
!pip install nltk textblob wordcloud --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob

nltk.download('stopwords')
nltk.download('wordnet')

# 2. Load data
df = pd.read_csv("/content/3) Sentiment dataset.csv")
print("Sentiment dataset:")
print(df[["Text", "Sentiment"]].head())

# 3. Preprocessing: tokenisasi, hapus stopwords, dan lemmatization
def preprocess(text):
    # Konversi ke lowercase
    text = str(text).lower()
    # Tokenisasi sederhana : split by space
    tokens = text.split()
    # Hapus stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return ' '.join(tokens)

df['Clean_Text'] = df['Text'].apply(preprocess)

print('\nContoh hasil preprocessing:')
print(df[['Text', 'Clean_Text']].head())

# 4. Analisis Sentimen (menggunakan TextBlob)
def polarity_tb(text):
    return TextBlob(text).sentiment.polarity

def get_tb_sentiment(polarity):
    if polarity > 0.1:
        return 'Positive'
    elif polarity < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

df['TB_Polarity'] = df['Clean_Text'].apply(polarity_tb)
df['TB_Sentiment'] = df['TB_Polarity'].apply(get_tb_sentiment)

# Tampilkan perbandingan label asli dan hasil analisis
print('\nPerbandingan Sentiment Data Asli & TextBlob:')
print(df[['Text','Sentiment','TB_Sentiment']].head(10))

# 5. Visualisasi distribusi sentimen (label asli & hasil analisis)
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

df['Sentiment'].value_counts().plot(kind='bar', ax=axs[0], title='Distribusi Sentimen Asli')
df['TB_Sentiment'].value_counts().plot(kind='bar', ax=axs[1], title='Distribusi Sentimen (TextBlob)')

plt.tight_layout()
plt.show()

# 6. Word Cloud per kelas sentimen
for sentiment_label in df['Sentiment'].unique():
    text_wc = ' '.join(df[df['Sentiment']==sentiment_label]['Clean_Text'])
    if text_wc.strip() != '':
        wc = WordCloud(width=400, height=300, background_color='white').generate(text_wc)
        plt.figure(figsize=(6, 4))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Word Cloud untuk Sentimen: {sentiment_label}')
        plt.show()

# 7. Tools yang digunakan
print("Tools: Python, nltk, TextBlob, pandas, matplotlib, wordcloud")

"""# **VERSI 2 NLP**"""

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re # Regular expression for text cleaning

# NLTK for text preprocessing
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# TextBlob for sentiment analysis
from textblob import TextBlob
# WordCloud for visualization
from wordcloud import WordCloud

# Download necessary NLTK data (run once)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

# 2. Load data
df = pd.read_csv("/content/3) Sentiment dataset.csv")
print("Sentiment dataset loaded successfully.")
print("Initial DataFrame head:")
print(df.head())
print("\nInitial Sentiment distribution:")
print(df['Sentiment'].value_counts())

# Rename 'Text' column to 'text' for consistency if needed
if 'Text' in df.columns:
    df.rename(columns={'Text': 'text'}, inplace=True)
# Ensure 'text' column exists
if 'text' not in df.columns:
    print("Error: 'text' column not found in the dataset. Please check your CSV file.")
    exit()

# 3. Text Preprocessing
print("\nStarting text preprocessing...")
# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = str(text).lower()
    # Remove special characters, numbers, and punctuation
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenization
    tokens = word_tokenize(text)
    # Remove stopwords and perform lemmatization
    processed_tokens = [
        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words
    ]
    # Join tokens back into a string
    return ' '.join(processed_tokens)

# Download punkt tokenizer if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

df['cleaned_text'] = df['text'].apply(preprocess_text)
print("Text preprocessing complete. Cleaned text example:")
print(df['cleaned_text'].head())

# 4. Sentiment Analysis using TextBlob
print("\nPerforming sentiment analysis with TextBlob...")
def get_textblob_sentiment(text):
    analysis = TextBlob(text)
    # Classify sentiment based on polarity
    if analysis.polarity > 0:
        return 'Positive'
    elif analysis.polarity < 0:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment analysis to the cleaned text
df['textblob_sentiment'] = df['cleaned_text'].apply(get_textblob_sentiment)
print("Sentiment analysis complete. TextBlob sentiment example:")
print(df[['text', 'textblob_sentiment']].head())

# 5. Visualize Sentiment Distribution
print("\nVisualizing sentiment distribution...")
plt.figure(figsize=(8, 6))
sns.countplot(x='textblob_sentiment', data=df, palette='viridis', order=['Positive', 'Neutral', 'Negative'])
plt.title('Sentiment Distribution (TextBlob)')
plt.xlabel('Sentiment')
plt.ylabel('Number of Texts')
plt.show()

# Compare with original sentiment if available
if 'Sentiment' in df.columns:
    print("\nOriginal Sentiment Distribution:")
    print(df['Sentiment'].value_counts())

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    sns.countplot(x='Sentiment', data=df, palette='coolwarm', order=df['Sentiment'].value_counts().index)
    plt.title('Original Sentiment Distribution')
    plt.xlabel('Sentiment')
    plt.ylabel('Number of Texts')

    plt.subplot(1, 2, 2)
    sns.countplot(x='textblob_sentiment', data=df, palette='viridis', order=['Positive', 'Neutral', 'Negative'])
    plt.title('TextBlob Sentiment Distribution')
    plt.xlabel('Sentiment')
    plt.ylabel('Number of Texts')
    plt.tight_layout()
    plt.show()

     # Optional: Cross-tabulation to see agreement
    print("\nCross-tabulation of Original vs. TextBlob Sentiment:")
    print(pd.crosstab(df['Sentiment'], df['textblob_sentiment']))

# 6. Visualize Word Frequencies using Word Clouds
print("\nGenerating Word Clouds for each sentiment category...")

# Combine all cleaned text for each sentiment category
positive_text = " ".join(df[df['textblob_sentiment'] == 'Positive']['cleaned_text'])
negative_text = " ".join(df[df['textblob_sentiment'] == 'Negative']['cleaned_text'])
neutral_text = " ".join(df[df['textblob_sentiment'] == 'Neutral']['cleaned_text'])

# Generate Word Clouds
def generate_wordcloud(text, title, color_map='viridis'):
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap=color_map).generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

if positive_text:
    generate_wordcloud(positive_text, 'Word Cloud for Positive Sentiment', 'Greens')
else:
    print("No positive text found for word cloud.")

if negative_text:
    generate_wordcloud(negative_text, 'Word Cloud for Negative Sentiment', 'Reds')
else:
    print("No negative text found for word cloud.")

if neutral_text:
    generate_wordcloud(neutral_text, 'Word Cloud for Neutral Sentiment', 'Blues')
else:
    print("No neutral text found for word cloud.")

print("\nSentiment analysis and visualization complete.")



